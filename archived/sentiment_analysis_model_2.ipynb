{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.read_csv('csv_files/cleaned_items_reviews_df.csv',index_col=0)\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop rows where cleaned_reviews are na\n",
    "final_df.isna()\n",
    "final_df = final_df.dropna(subset=['cleaned_reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['user_rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "final_df['user_rating'].value_counts().plot.bar(rot=0)\n",
    "\n",
    "plt.title('Count of User Rating',fontsize=20)\n",
    "plt.xlabel('Rating',fontsize=15)\n",
    "plt.xticks(fontsize=10)\n",
    "plt.ylabel('Count',fontsize=15)\n",
    "plt.yticks(fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we have a class imbalance problem where the number of 5.0 reviews are much greater than the other reviews. Oversampling the minority classes will lead to overfitting due to a creation of large number of synthetic rows. To address this, I will take a set number of random samples from each class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Undersampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undersampling(dataframe, column, list_user_rating, samples):\n",
    "    df = pd.DataFrame()\n",
    "    for rating in list_user_rating:\n",
    "        sample = dataframe[dataframe[column] == rating].sample(samples)\n",
    "        df = df.append(sample)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "undersampled_df =undersampling(final_df, 'user_rating', final_df['user_rating'].unique(), 250)\n",
    "undersampled_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Vectorization\n",
    "\n",
    "Stands for Term Frequency, Inverse Document Frequency and weighs each word in a document by how unique it is. The idea behind TF-IDF is that the words that occur less in all the documents and more in individual documents are weighted more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare variables for reviews and rating\n",
    "reviews = undersampled_df['cleaned_reviews'].values\n",
    "rating = undersampled_df['user_rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert text into TF-IDF feature vectors\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=2500, # num of most frequent occuring words to create bag of words vector\n",
    "                             min_df=7, # minimum words that occur in at least 7 documents\n",
    "                             max_df=0.8, # words that occur in a maximum of 80% of documents\n",
    "                             stop_words=stopwords.words('english'))\n",
    "\n",
    "processed_features = vectorizer.fit_transform(reviews).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vectorizer)\n",
    "print(processed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(processed_features,\n",
    "                                                    rating, \n",
    "                                                    test_size=0.2, # 80% training/20% test\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform a gridsearch for Random Forest\n",
    "# # Obtain optimal values of model hyperparameters\n",
    "\n",
    "# RF_classifier = RandomForestClassifier(random_state=0)\n",
    "\n",
    "# params = {'criterion': ['gini','entropy'], # Measure the quality of a split\n",
    "#              'max_depth': [10, 20, 30], # Maximum number of levels in tree\n",
    "#              'max_features': ['auto', 'sqrt'], # Number of features to consider at every split\n",
    "#              'min_samples_leaf': [1, 2, 4], # Minimum number of samples required at each leaf node\n",
    "#              'min_samples_split': [2, 5, 10], # Minimum number of samples required to split a node\n",
    "#              'n_estimators': [200, 500, 1000]} # The number of trees in the forest\n",
    "\n",
    "# g_s_RF = GridSearchCV(RF_classifier,param_grid=params, n_jobs=-1) # create gridsearch with input params\n",
    "# grid_result = g_s_RF.fit(X_train, y_train) # fit gridsearch with training data\n",
    "# best_params = grid_result.best_params_ # output best params for given data\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_classifier = RandomForestClassifier(criterion='gini',\n",
    "                                       max_depth=10,\n",
    "                                       max_features='auto',\n",
    "                                       min_samples_leaf=4,\n",
    "                                       min_samples_split=2,\n",
    "                                       n_estimators=1000, \n",
    "                                       random_state=0)\n",
    "RF_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF_predictions = RF_classifier.predict(X_test)\n",
    "RF_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, RF_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, RF_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, RF_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = pd.DataFrame({'Model':['Random Forest'], 'Accuracy':[accuracy_score(y_test, RF_predictions)]})\n",
    "accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comparison between Actuals and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_count = dict(Counter(RF_predictions))\n",
    "predictions_count_df = pd.DataFrame.from_dict(predictions_count, orient ='index', columns =['Predictions'])\n",
    "\n",
    "y_test_count = dict(Counter(y_test))\n",
    "y_test_count_df = pd.DataFrame.from_dict(y_test_count, orient ='index', columns =['Actuals'])\n",
    "\n",
    "random_forest_df = pd.concat([predictions_count_df, y_test_count_df], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_df.plot.bar(rot=0, figsize=(20,10))\n",
    "\n",
    "plt.title('Comparison between Actual Reviews and Predicted Reviews (Random Forest)',fontsize=30)\n",
    "\n",
    "plt.xlabel('Rating',fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform a gridsearch for Random Forest\n",
    "# # Obtain optimal values of model hyperparameters\n",
    "\n",
    "# KNN_classifier = KNeighborsClassifier()\n",
    "\n",
    "# params = {'n_neighbors': [5,15,25], # Number of neighbors\n",
    "#              'weights': ['uniform','distance']} # Weight function used in prediction\n",
    "\n",
    "# g_s_KNN = GridSearchCV(KNN_classifier,param_grid=params, n_jobs=-1) # create gridsearch with input params\n",
    "# grid_result = g_s_KNN.fit(X_train, y_train) # fit gridsearch with training data\n",
    "# best_params = grid_result.best_params_ # output best params for given data\n",
    "# best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_classifier = KNeighborsClassifier(n_neighbors=15,weights='distance')\n",
    "KNN_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNN_predictions = KNN_classifier.predict(X_test)\n",
    "KNN_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, KNN_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, KNN_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, KNN_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_results = accuracy_results.append({'Model':'KNN', 'Accuracy':accuracy_score(y_test, KNN_predictions)},ignore_index=True)\n",
    "accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comparison between Actuals and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_count = dict(Counter(KNN_predictions))\n",
    "predictions_count_df = pd.DataFrame.from_dict(predictions_count, orient ='index', columns =['Predictions'])\n",
    "\n",
    "y_test_count = dict(Counter(y_test))\n",
    "y_test_count_df = pd.DataFrame.from_dict(y_test_count, orient ='index', columns =['Actuals'])\n",
    "\n",
    "KNN_df = pd.concat([predictions_count_df, y_test_count_df], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_df.plot.bar(rot=0, figsize=(20,10))\n",
    "\n",
    "plt.title('Comparison between Actual Reviews and Predicted Reviews (KNN)',fontsize=30)\n",
    "\n",
    "plt.xlabel('Rating',fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_classifier = SVC(kernel='linear')\n",
    "SVC_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_predictions = SVC_classifier.predict(X_test)\n",
    "SVC_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_test, SVC_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, SVC_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, SVC_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_results = accuracy_results.append({'Model':'SVC', 'Accuracy':accuracy_score(y_test, SVC_predictions)},ignore_index=True)\n",
    "accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comparison between Actuals and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_count = dict(Counter(SVC_predictions))\n",
    "predictions_count_df = pd.DataFrame.from_dict(predictions_count, orient ='index', columns =['Predictions'])\n",
    "\n",
    "y_test_count = dict(Counter(y_test))\n",
    "y_test_count_df = pd.DataFrame.from_dict(y_test_count, orient ='index', columns =['Actuals'])\n",
    "\n",
    "KNN_df = pd.concat([predictions_count_df, y_test_count_df], axis=1).fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forest_df.plot.bar(rot=0, figsize=(20,10))\n",
    "\n",
    "plt.title('Comparison between Actual Reviews and Predicted Reviews (KNN)',fontsize=30)\n",
    "\n",
    "plt.xlabel('Rating',fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analyzer using NLTK Module Vader\n",
    "\n",
    "* works well with social media type text\n",
    "* doesn't require any training data and is constructed using human-curated sentiment lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyser = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(sentence):\n",
    "    # polarity_scores: ranging from -1 (most neg) to 1 (most pos)\n",
    "    score = analyser.polarity_scores(sentence)\n",
    "    return score # {neg: Negative, neu: Neutral, pos: Positive, compound: Aggregated Score}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compound_score(text):\n",
    "    comp=sentiment_analyzer_scores(text)\n",
    "    return comp['compound'] # returns the compound score from the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying on the reviews column to get the score\n",
    "final_df['sentiment_score'] = final_df['review_description'].apply(lambda x:compound_score(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_category(score):\n",
    "    if score >= 0.6:\n",
    "        return 5.0\n",
    "    elif score >= 0.2:\n",
    "        return 4.0\n",
    "    elif score >= -0.2:\n",
    "        return 3.0\n",
    "    elif score >= -0.6:\n",
    "        return 2.0\n",
    "    else:\n",
    "        return 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df['sentiment_score_rating'] = final_df['sentiment_score'].apply(lambda x: sentiment_category(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnf_matrix = confusion_matrix(final_df['user_rating'],final_df['sentiment_score_rating'])\n",
    "cnf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(final_df['user_rating'],final_df['sentiment_score_rating']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "vader_accuracy_score = accuracy_score(final_df['user_rating'],final_df['sentiment_score_rating'])\n",
    "vader_accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results = accuracy_results.append({'Model':'Vader', 'Accuracy':vader_accuracy_score},ignore_index=True)\n",
    "accuracy_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Comparison between Actuals and Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "review_rating_counts = final_df['user_rating'].value_counts()\n",
    "sentiment_score_rating_counts = final_df['sentiment_score_rating'].value_counts()\n",
    "\n",
    "review_df = pd.concat([review_rating_counts, sentiment_score_rating_counts], axis=1)\n",
    "\n",
    "review_df.plot.bar(rot=0, figsize=(20,10))\n",
    "\n",
    "plt.title('Comparison between Actual Reviews and Predicted Reviews',fontsize=30)\n",
    "\n",
    "plt.xlabel('Rating',fontsize=25)\n",
    "plt.xticks(fontsize=20)\n",
    "plt.ylabel('Count',fontsize=25)\n",
    "plt.yticks(fontsize=20)\n",
    "plt.legend(fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictive Accuracy Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:learn-env] *",
   "language": "python",
   "name": "conda-env-learn-env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
